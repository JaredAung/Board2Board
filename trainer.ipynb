{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Board2Board Trainer**\n",
        "**Approach**\n",
        "\n",
        "\n",
        "1.   The training pipeline consists of two core components:\n",
        "Piece Classification: A ResNet50 model (pretrained on ImageNet) is fine-tuned to classify chessboard squares into 13 categories. Input images (64×64 crops) are resized to 224×224 to match the ResNet50 architecture requirements. Dataset splitting is performed with splitfolders to create training and validation sets, and training is regularized using data augmentation, early stopping, and learning rate reduction.\n",
        "2.   Threshold Prediction: Separate Linear Regression models are trained on hand-engineered image features to predict optimal thresholds for board detection (Hough Line threshold and Agglomerative Clustering threshold, both in primary and secondary processing stages). These thresholds are later integrated into the OpenCV-based board processor for robust grid extraction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vIRBY17MJHcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Libraries that require installing*"
      ],
      "metadata": {
        "id": "dUp5muWNSRwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install split-folders"
      ],
      "metadata": {
        "id": "ckdUMJphKfei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50 Fine-Tuning\n",
        "A transfer learning setup is used:\n",
        "* The pretrained ResNet50 backbone is loaded with ImageNet weights, excluding its top classification layers.\n",
        "* All layers are frozen initially, then the last 10 layers are unfrozen for fine-tuning.\n",
        "* A custom head is attached: convolutional outputs are flattened, followed by a dense layer with 128 ReLU units, and a final softmax layer that outputs predictions for 13 classes (piece types + empty square).\n",
        "* The model is compiled with the Adam optimizer and categorical cross-entropy loss.\n",
        "* Training uses an 80/20 split with real-time data augmentation. Early stopping (based on validation accuracy) and ReduceLROnPlateau (on validation loss) ensure convergence without overfitting."
      ],
      "metadata": {
        "id": "kybvSWu4SdXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqkHjugLlvRm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense,GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import splitfolders\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Creating the Neural Network\n",
        "\n",
        "# Loading the model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Transfer Learning and Fine Tuning the Model on the custom dataset\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "#base_model.summary()\n",
        "\n",
        "for layer in base_model.layers[-10:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "# Create the final model\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(13, activation='softmax')(x) #classify into 13 classes\n",
        "\n",
        "output_layer = predictions\n",
        "\n",
        "# Compiling the model\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Training the model\n",
        "\n",
        "# split the dataset: 80% training data and 20% validation data\n",
        "splitfolders.ratio(\"/content/drive/MyDrive/dataset2\",output=\"output_dataset\", seed=42, ratio=(0.8, 0.2))\n",
        "\n",
        "train_dir = \"output_dataset/train\"\n",
        "val_dir = \"output_dataset/val\"\n",
        "\n",
        "# Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=[0.8, 1.2])\n",
        "\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "# 224x224 images are most suitable with ResNet50 model\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# Early Stopping: check the validation accuracy\n",
        "# if validation accuracy doesn't improve after 4 epoches, stop\n",
        "# and restore the weights calculated in the highest accuracy epoch\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=4,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Reduce Learning Rate where there is no improvement in validation loss\n",
        "# fir 3 epoches\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Start training\n",
        "history = model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=[early_stop,reduce_lr])\n",
        "\n",
        "# Metrics\n",
        "loss, accuracy = model.evaluate(val_gen)\n",
        "print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n",
        "\n",
        "# Save model\n",
        "model.save(\"board2board.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Evaluation: Confusion Matrix**\n",
        "\n",
        "To assess classification performance:\n",
        "* A confusion matrix is computed by comparing predicted labels against true labels from the validation set.\n",
        "* The results are visualized as a heatmap (using Seaborn) with class labels on both axes.\n",
        "* A classification_report provides precision, recall, and F1 scores for each class. This combination allows both high-level accuracy monitoring and detailed insight into misclassifications between specific piece types."
      ],
      "metadata": {
        "id": "H2_KZAySKni5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## Confusion Matrix to ensure model is well trained\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for i in range(len(val_gen)):\n",
        "    x_batch, y_batch = val_gen[i]\n",
        "    y_true_batch = np.argmax(y_batch, axis=1)\n",
        "    y_pred_batch = np.argmax(model.predict(x_batch), axis=1)\n",
        "\n",
        "    y_true.extend(y_true_batch)\n",
        "    y_pred.extend(y_pred_batch)\n",
        "\n",
        "    if i >= val_gen.__len__():\n",
        "        break\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=val_gen.class_indices.keys()))\n"
      ],
      "metadata": {
        "id": "atS6ZXoLLUgf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression for Threshold Prediction\n",
        "\n",
        "While CNNs handle piece recognition, board detection relies on image-specific thresholds. To avoid manually tuning them, regression models are trained to predict thresholds directly from image features.\n",
        "\n",
        "**Pipeline**\n",
        "1.  Dataset Preparation: Each dataset (e.g., primary_hough_training.csv) contains numerical image descriptors (intensity, entropy, edge density, etc.) and the corresponding optimal threshold.\n",
        "Train-Test Split: Data is divided into 80% training and 20% testing.\n",
        "Feature Scaling: Standardization ensures features have zero mean and unit variance.\n",
        "2.  Model Training: A LinearRegression model is fit on the scaled features.\n",
        "3.  Evaluation: Predictions are compared against ground truth using MSE and R² metrics. Scatter plots visualize predicted vs. true thresholds.\n",
        "4.  Pipeline Export: A scikit-learn pipeline (scaler + regression model) is serialized with Joblib for integration into the OpenCV board processor.\n",
        "\n",
        "**Separate Models**\n",
        "1.  *Primary Houghline Threshold* — predicts the line detection threshold for initial board extraction.\n",
        "2.  *Primary Clustering Threshold* — predicts the distance parameter for clustering intersection points into grid corners.\n",
        "3.  *Secondary Houghline Threshold* — refines thresholds after perspective correction.\n",
        "4.  *Secondary Clustering Threshold* — adjusts clustering in the warped image stage."
      ],
      "metadata": {
        "id": "M7yp5Sf-LuN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primary Houghline Threshold Linear Regression Pipeline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Load Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Board2Board/primary_hough_training.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"hough_threshold\", axis=1).values\n",
        "y = df[\"hough_threshold\"].values.reshape(-1, 1)\n",
        "\n",
        "print(\"Feature columns:\", df.columns.tolist())\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Standardize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(\"Learned Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# 6. Plot\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
        "plt.xlabel(\"True Hough Threshold\")\n",
        "plt.ylabel(\"Predicted Hough Threshold\")\n",
        "plt.title(\"Linear Regression: True vs Predicted\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 7. Save Model\n",
        "pipeline = make_pipeline(scaler, model)\n",
        "joblib.dump(pipeline, \"primary_hough_pipeline.pkl\")\n"
      ],
      "metadata": {
        "id": "g14uGZKHEXMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primary Clustering Threshold Linear Regression Pipeline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Load Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Board2Board/primary_clustering_training.csv\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\" cluster_threshold\", axis=1).values\n",
        "y = df[\" cluster_threshold\"].values.reshape(-1, 1)\n",
        "\n",
        "print(\"Feature columns:\", df.columns.tolist())\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Standardize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(\"Learned Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# 6. Plot\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
        "plt.xlabel(\"True Cluster Threshold\")\n",
        "plt.ylabel(\"Predicted Cluster Threshold\")\n",
        "plt.title(\"Linear Regression: True vs Predicted\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 7. Save Model\n",
        "pipeline = make_pipeline(scaler, model)\n",
        "joblib.dump(pipeline, \"primary_cluster_pipeline.pkl\")\n"
      ],
      "metadata": {
        "id": "cHTNUDUTH1XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secondary Houghline Threshold Linear Regression Pipeline\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Load Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Board2Board/secondary_hough_training.txt\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"hough_threshold\", axis=1).values\n",
        "y = df[\"hough_threshold\"].values.reshape(-1, 1)\n",
        "\n",
        "print(\"Feature columns:\", df.columns.tolist())\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Standardize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(\"Learned Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# 6. Plot\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
        "plt.xlabel(\"True Houghline Threshold\")\n",
        "plt.ylabel(\"Predicted Houghline Threshold\")\n",
        "plt.title(\"Linear Regression: True vs Predicted\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 7. Save Model\n",
        "pipeline = make_pipeline(scaler, model)\n",
        "joblib.dump(pipeline, \"secondary_hough_pipeline.pkl\")\n"
      ],
      "metadata": {
        "id": "k1kqJD31I1X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secondary Clustering Threshold Linear Regression Pipeline\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Load Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Board2Board/secondary_clustering_training.txt\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\" cluster_threshold\", axis=1).values\n",
        "y = df[\" cluster_threshold\"].values.reshape(-1, 1)\n",
        "\n",
        "print(\"Feature columns:\", df.columns.tolist())\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Standardize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(\"Learned Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# 6. Plot\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
        "plt.xlabel(\"True Cluster Threshold\")\n",
        "plt.ylabel(\"Predicted Cluster Threshold\")\n",
        "plt.title(\"Linear Regression: True vs Predicted\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 7. Save Model\n",
        "pipeline = make_pipeline(scaler, model)\n",
        "joblib.dump(pipeline, \"secondary_cluster_pipeline.pkl\")\n"
      ],
      "metadata": {
        "id": "hXtTQqXkJJGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**© 2025 Jared Aung. All rights reserved.**\n"
      ],
      "metadata": {
        "id": "hqQJlRTGU7hX"
      }
    }
  ]
}
